{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.utils import generic_utils\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation, Lambda, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Deconv2D, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "import keras.backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetpath = './output/datasetimages.hdf5'\n",
    "patch_size = 32\n",
    "batch_size = 16\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    return X / 127.5 - 1\n",
    "\n",
    "def load_data(datasetpath):\n",
    "    with h5py.File(datasetpath, \"r\") as hf:\n",
    "        X_full_train = hf[\"train_data_raw\"][:].astype(np.float32)\n",
    "        X_full_train = normalization(X_full_train)\n",
    "        X_sketch_train = hf[\"train_data_gen\"][:].astype(np.float32)\n",
    "        X_sketch_train = normalization(X_sketch_train)\n",
    "        X_full_val = hf[\"val_data_raw\"][:].astype(np.float32)\n",
    "        X_full_val = normalization(X_full_val)\n",
    "        X_sketch_val = hf[\"val_data_gen\"][:].astype(np.float32)\n",
    "        X_sketch_val = normalization(X_sketch_val)\n",
    "        return X_full_train, X_sketch_train, X_full_val, X_sketch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_unet(x, f, name, bn_axis, bn=True, strides=(2,2)):\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(f, (3,3), strides=strides, name=name, padding='same')(x)\n",
    "    if bn: x = BatchNormalization(axis=bn_axis)(x)\n",
    "    return x\n",
    "\n",
    "def up_conv_block_unet(x, x2, f, name, bn_axis, bn=True, dropout=False):\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(f, (3,3), name=name, padding='same')(x)\n",
    "    if bn: x = BatchNormalization(axis=bn_axis)(x)\n",
    "    if dropout: x = Dropout(0.5)(x)\n",
    "    x = Concatenate(axis=bn_axis)([x, x2])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_unet_upsampling(img_shape, disc_img_shape, model_name=\"generator_unet_upsampling\"):\n",
    "    filters_num = 64\n",
    "    axis_num = -1\n",
    "    channels_num = img_shape[-1]\n",
    "    min_s = min(img_shape[:-1])\n",
    "\n",
    "    unet_input = Input(shape=img_shape, name=\"unet_input\")\n",
    "\n",
    "    conv_num = int(np.floor(np.log(min_s)/np.log(2)))\n",
    "    list_filters_num = [filters_num*min(8, (2**i)) for i in range(conv_num)]\n",
    "\n",
    "    # Encoder\n",
    "    first_conv = Conv2D(list_filters_num[0], (3,3), strides=(2,2), name='unet_conv2D_1', padding='same')(unet_input)\n",
    "    list_encoder = [first_conv]\n",
    "    for i, f in enumerate(list_filters_num[1:]):\n",
    "        name = 'unet_conv2D_' + str(i+2)\n",
    "        conv = conv_block_unet(list_encoder[-1], f, name, axis_num)\n",
    "        list_encoder.append(conv)\n",
    "\n",
    "    # prepare decoder filters\n",
    "    list_filters_num = list_filters_num[:-2][::-1]\n",
    "    if len(list_filters_num) < conv_num-1:\n",
    "        list_filters_num.append(filters_num)\n",
    "\n",
    "    # Decoder\n",
    "    first_up_conv = up_conv_block_unet(list_encoder[-1], list_encoder[-2],\n",
    "                        list_filters_num[0], \"unet_upconv2D_1\", axis_num, dropout=True)\n",
    "    list_decoder = [first_up_conv]\n",
    "    for i, f in enumerate(list_filters_num[1:]):\n",
    "        name = \"unet_upconv2D_\" + str(i+2)\n",
    "        if i<2:\n",
    "            d = True\n",
    "        else:\n",
    "            d = False\n",
    "        up_conv = up_conv_block_unet(list_decoder[-1], list_encoder[-(i+3)], f, name, axis_num, dropout=d)\n",
    "        list_decoder.append(up_conv)\n",
    "\n",
    "    x = Activation('relu')(list_decoder[-1])\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(disc_img_shape[-1], (3,3), name=\"last_conv\", padding='same')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    generator_unet = Model(inputs=[unet_input], outputs=[x])\n",
    "    return generator_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCGAN_discriminator(img_shape, disc_img_shape, patch_num, model_name='DCGAN_discriminator'):\n",
    "    disc_raw_img_shape = (disc_img_shape[0], disc_img_shape[1], img_shape[-1])\n",
    "    list_input = [Input(shape=disc_img_shape, name='disc_input_'+str(i)) for i in range(patch_num)]\n",
    "    list_raw_input = [Input(shape=disc_raw_img_shape, name='disc_raw_input_'+str(i)) for i in range(patch_num)]\n",
    "\n",
    "    axis_num = -1\n",
    "    filters_num = 64\n",
    "    conv_num = int(np.floor(np.log(disc_img_shape[1])/np.log(2)))\n",
    "    list_filters = [filters_num*min(8, (2**i)) for i in range(conv_num)]\n",
    "    print(\"DCGAN_First Conv\")\n",
    "    generated_patch_input = Input(shape=disc_img_shape, name='discriminator_input')\n",
    "    xg = Conv2D(list_filters[0], (3,3), strides=(2,2), name='disc_conv2d_1', padding='same')(generated_patch_input)\n",
    "    xg = BatchNormalization(axis=axis_num)(xg)\n",
    "    xg = LeakyReLU(0.2)(xg)\n",
    "\n",
    "    print(\"DCGAN_First Raw Conv\")\n",
    "    raw_patch_input = Input(shape=disc_raw_img_shape, name='discriminator_raw_input')\n",
    "    xr = Conv2D(list_filters[0], (3,3), strides=(2,2), name='raw_disc_conv2d_1', padding='same')(raw_patch_input)\n",
    "    xr = BatchNormalization(axis=axis_num)(xr)\n",
    "    xr = LeakyReLU(0.2)(xr)\n",
    "\n",
    "    print(\"DCGAN_Next Conv\")\n",
    "    for i, f in enumerate(list_filters[1:]):\n",
    "        name = 'disc_conv2d_' + str(i+2)\n",
    "        x = Concatenate(axis=axis_num)([xg, xr])\n",
    "        x = Conv2D(f, (3,3), strides=(2,2), name=name, padding='same')(x)\n",
    "        x = BatchNormalization(axis=axis_num)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "    x_flat = Flatten()(x)\n",
    "    x = Dense(2, activation='softmax', name='disc_dense')(x_flat)\n",
    "    \n",
    "    PatchGAN = Model(inputs=[generated_patch_input, raw_patch_input], outputs=[x], name='PatchGAN')\n",
    "    print(\"DCGAN_PATCH_COMPLETE\")\n",
    "    x = [PatchGAN([list_input[i], list_raw_input[i]]) for i in range(patch_num)]\n",
    "    if len(x)>1:\n",
    "        x = Concatenate(axis=axis_num)(x)\n",
    "    else:\n",
    "        x = x[0]\n",
    "    x_out = Dense(2, activation='softmax', name='disc_output')(x)\n",
    "    discriminator_model = Model(inputs=(list_input+list_raw_input), outputs=[x_out], name=model_name)\n",
    "    print(\"DCGAN_END\")\n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCGAN(generator, discriminator, img_shape, patch_size):\n",
    "    raw_input = Input(shape=img_shape, name='DCGAN_input')\n",
    "    genarated_image = generator(raw_input)\n",
    "\n",
    "    h, w = img_shape[:-1]\n",
    "    ph, pw = patch_size, patch_size\n",
    "\n",
    "    list_row_idx = [(i*ph, (i+1)*ph) for i in range(h//ph)]\n",
    "    list_col_idx = [(i*pw, (i+1)*pw) for i in range(w//pw)]\n",
    "\n",
    "    list_gen_patch = []\n",
    "    list_raw_patch = []\n",
    "    for row_idx in list_row_idx:\n",
    "        for col_idx in list_col_idx:\n",
    "            raw_patch = Lambda(lambda z: z[:, row_idx[0]:row_idx[1], col_idx[0]:col_idx[1], :])(raw_input)\n",
    "            list_raw_patch.append(raw_patch)\n",
    "            x_patch = Lambda(lambda z: z[:, row_idx[0]:row_idx[1], col_idx[0]:col_idx[1], :])(genarated_image)\n",
    "            list_gen_patch.append(x_patch)\n",
    "\n",
    "    DCGAN_output = discriminator(list_gen_patch+list_raw_patch)\n",
    "\n",
    "    DCGAN = Model(inputs=[raw_input],\n",
    "                  outputs=[genarated_image, DCGAN_output],\n",
    "                  name='DCGAN')\n",
    "\n",
    "    return DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generator(img_shape, disc_img_shape):\n",
    "    model = generator_unet_upsampling(img_shape, disc_img_shape)\n",
    "    return model\n",
    "\n",
    "def load_DCGAN_discriminator(img_shape, disc_img_shape, patch_num):\n",
    "    model = DCGAN_discriminator(img_shape, disc_img_shape, patch_num)\n",
    "    return model\n",
    "\n",
    "def load_DCGAN(generator, discriminator, img_shape, patch_size):\n",
    "    model = DCGAN(generator, discriminator, img_shape, patch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(y_true, y_pred):\n",
    "    return K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "\n",
    "def inverse_normalization(X):\n",
    "    return (X + 1.) / 2.\n",
    "\n",
    "def to3d(X):\n",
    "    if X.shape[-1]==3: return X\n",
    "    b = X.transpose(3,1,2,0)\n",
    "    c = np.array([b[0],b[0],b[0]])\n",
    "    return c.transpose(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_batch(X_proc, X_raw, generator_model, batch_size, suffix):\n",
    "\n",
    "    X_gen = generator_model.predict(X_raw)\n",
    "    X_raw = inverse_normalization(X_raw)\n",
    "    X_proc = inverse_normalization(X_proc)\n",
    "    X_gen = inverse_normalization(X_gen)\n",
    "\n",
    "    Xs = to3d(X_raw[:5])\n",
    "    Xg = to3d(X_gen[:5])\n",
    "    Xr = to3d(X_proc[:5])\n",
    "    Xs = np.concatenate(Xs, axis=1)\n",
    "    Xg = np.concatenate(Xg, axis=1)\n",
    "    Xr = np.concatenate(Xr, axis=1)\n",
    "    XX = np.concatenate((Xs,Xg,Xr),axis = 0)\n",
    "    plt.imshow(XX)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"output/batch_\"+suffix+\"/x_0.jpg\")\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(X, patch_size):\n",
    "    list_X = []\n",
    "    list_row_idx = [(i*patch_size, (i+1)*patch_size) for i in range(X.shape[1] // patch_size)]\n",
    "    list_col_idx = [(i*patch_size, (i+1)*patch_size) for i in range(X.shape[2] // patch_size)]\n",
    "    for row_idx in list_row_idx:\n",
    "        for col_idx in list_col_idx:\n",
    "            list_X.append(X[:, row_idx[0]:row_idx[1], col_idx[0]:col_idx[1], :])\n",
    "    return list_X\n",
    "\n",
    "def get_disc_batch(procImage, rawImage, generator_model, batch_counter, patch_size):\n",
    "    if batch_counter % 2 == 0:\n",
    "        # produce an output\n",
    "        X_disc = generator_model.predict(rawImage)\n",
    "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
    "        y_disc[:, 0] = 1\n",
    "    else:\n",
    "        X_disc = procImage\n",
    "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
    "\n",
    "    X_disc = extract_patches(X_disc, patch_size)\n",
    "    return X_disc, y_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"load data\")\n",
    "    rawImage, procImage, rawImage_val, procImage_val = load_data(datasetpath)\n",
    "\n",
    "    img_shape = rawImage.shape[-3:]\n",
    "    patch_num = (img_shape[0] // patch_size) * (img_shape[1] // patch_size)\n",
    "    disc_img_shape = (patch_size, patch_size, procImage.shape[-1])\n",
    "\n",
    "    print(\"train\")\n",
    "    opt_dcgan = Adam(lr=1E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt_discriminator = Adam(lr=1E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    print(\"load generator model\")\n",
    "    generator_model = load_generator(img_shape, disc_img_shape)\n",
    "    print(\"load discriminator model\")\n",
    "    discriminator_model = load_DCGAN_discriminator(img_shape, disc_img_shape, patch_num)\n",
    "    \n",
    "    generator_model.compile(loss='mae', optimizer=opt_discriminator)\n",
    "    discriminator_model.trainable = False\n",
    "    \n",
    "    DCGAN_model = load_DCGAN(generator_model, discriminator_model, img_shape, patch_size)\n",
    "    \n",
    "    loss = [l1_loss, 'binary_crossentropy']\n",
    "    loss_weights = [1E1, 1]\n",
    "    DCGAN_model.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_dcgan)\n",
    "    \n",
    "    discriminator_model.trainable = True\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=opt_discriminator)\n",
    "\n",
    "    # start training\n",
    "    print('start training')\n",
    "    for e in range(epoch):\n",
    "\n",
    "        starttime = time.time()\n",
    "        perm = np.random.permutation(rawImage.shape[0])\n",
    "        X_procImage = procImage[perm]\n",
    "        X_rawImage  = rawImage[perm]\n",
    "        X_procImageIter = [X_procImage[i:i+batch_size] for i in range(0, rawImage.shape[0], batch_size)]\n",
    "        X_rawImageIter  = [X_rawImage[i:i+batch_size] for i in range(0, rawImage.shape[0], batch_size)]\n",
    "        b_it = 0\n",
    "        progbar = generic_utils.Progbar(len(X_procImageIter)*batch_size)\n",
    "        for (X_proc_batch, X_raw_batch) in zip(X_procImageIter, X_rawImageIter):\n",
    "            b_it += 1\n",
    "            X_disc, y_disc = get_disc_batch(X_proc_batch, X_raw_batch, generator_model, b_it, patch_size)\n",
    "            raw_disc, _ = get_disc_batch(X_raw_batch, X_raw_batch, generator_model, 1, patch_size)\n",
    "            x_disc = X_disc + raw_disc\n",
    "            # update the discriminator\n",
    "            disc_loss = discriminator_model.train_on_batch(x_disc, y_disc)\n",
    "\n",
    "            # create a batch to feed the generator model\n",
    "            idx = np.random.choice(procImage.shape[0], batch_size)\n",
    "            X_gen_target, X_gen = procImage[idx], rawImage[idx]\n",
    "            y_gen = np.zeros((X_gen.shape[0], 2), dtype=np.uint8)\n",
    "            y_gen[:, 1] = 1\n",
    "\n",
    "            # Freeze the discriminator\n",
    "            discriminator_model.trainable = False\n",
    "            gen_loss = DCGAN_model.train_on_batch(X_gen, [X_gen_target, y_gen])\n",
    "            # Unfreeze the discriminator\n",
    "            discriminator_model.trainable = True\n",
    "\n",
    "            progbar.add(batch_size, values=[\n",
    "                (\"D logloss\", disc_loss),\n",
    "                (\"G tot\", gen_loss[0]),\n",
    "                (\"G L1\", gen_loss[1]),\n",
    "                (\"G logloss\", gen_loss[2])\n",
    "            ])\n",
    "\n",
    "            # save images for visualization\n",
    "            if b_it % (procImage.shape[0]//batch_size//2) == 0:\n",
    "                plot_generated_batch(X_proc_batch, X_raw_batch, generator_model, batch_size, \"training\")\n",
    "                idx = np.random.choice(procImage_val.shape[0], batch_size)\n",
    "                X_gen_target, X_gen = procImage_val[idx], rawImage_val[idx]\n",
    "                plot_generated_batch(X_gen_target, X_gen, generator_model, batch_size, \"validation\")\n",
    "        print()\n",
    "        print('Epoch %s/%s, Time: %s' % (e + 1, epoch, time.time() - starttime))\n",
    "    print(\"END_TRAINING\")\n",
    "  \n",
    "    imgs = generator_model.predict(rawImage_val)\n",
    "    for i,img in enumerate(imgs):\n",
    "        print(i)\n",
    "        pimg = inverse_normalization(procImage_val[i])\n",
    "        plt.imshow(pimg)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"output/testData/as\"+str(i)+\".jpg\")\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        img = inverse_normalization(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"output/testData/\"+str(i)+\".jpg\")\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    \n",
    "    t_imgs = generator_model.predict(rawImage)\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        img = inverse_normalization(t_imgs[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"output/T_Data/as\"+str(i)+\".jpg\")\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    #save_data\n",
    "    generator_model.save(\"output/generator_data/generator_model.h5\")\n",
    "    print(\"END_MAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "train\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "load generator model\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "load discriminator model\n",
      "DCGAN_First Conv\n",
      "DCGAN_First Raw Conv\n",
      "DCGAN_Next Conv\n",
      "DCGAN_PATCH_COMPLETE\n",
      "DCGAN_END\n",
      "start training\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "512/512 [==============================] - 37s 72ms/step - D logloss: 0.6952 - G tot: 14.0727 - G L1: 1.3359 - G logloss: 0.7137\n",
      "\n",
      "Epoch 1/100, Time: 36.84327816963196\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5857 - G tot: 12.6371 - G L1: 1.1575 - G logloss: 1.0617\n",
      "\n",
      "Epoch 2/100, Time: 14.925901174545288\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5652 - G tot: 12.0208 - G L1: 1.0762 - G logloss: 1.2587\n",
      "\n",
      "Epoch 3/100, Time: 14.919252157211304\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5627 - G tot: 11.6364 - G L1: 1.0296 - G logloss: 1.3401\n",
      "\n",
      "Epoch 4/100, Time: 14.902195930480957\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 11.8246 - G L1: 1.0454 - G logloss: 1.3702\n",
      "\n",
      "Epoch 5/100, Time: 14.923876762390137\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 11.2996 - G L1: 0.9919 - G logloss: 1.3809\n",
      "\n",
      "Epoch 6/100, Time: 14.940673589706421\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 11.6984 - G L1: 1.0314 - G logloss: 1.3845\n",
      "\n",
      "Epoch 7/100, Time: 15.029691219329834\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 10.6495 - G L1: 0.9264 - G logloss: 1.3857\n",
      "\n",
      "Epoch 8/100, Time: 14.96021294593811\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 10.4326 - G L1: 0.9047 - G logloss: 1.3861\n",
      "\n",
      "Epoch 9/100, Time: 14.944020509719849\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 10.5597 - G L1: 0.9173 - G logloss: 1.3862\n",
      "\n",
      "Epoch 10/100, Time: 14.95285701751709\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 10.2155 - G L1: 0.8829 - G logloss: 1.3863\n",
      "\n",
      "Epoch 11/100, Time: 14.893035411834717\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5625 - G tot: 9.8427 - G L1: 0.8456 - G logloss: 1.3863\n",
      "\n",
      "Epoch 12/100, Time: 15.03966498374939\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 9.6839 - G L1: 0.8298 - G logloss: 1.3863\n",
      "\n",
      "Epoch 13/100, Time: 14.983184814453125\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 9.3544 - G L1: 0.7968 - G logloss: 1.3863\n",
      "\n",
      "Epoch 14/100, Time: 14.975003242492676\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 9.5141 - G L1: 0.8128 - G logloss: 1.3863\n",
      "\n",
      "Epoch 15/100, Time: 14.909649133682251\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 9.2775 - G L1: 0.7891 - G logloss: 1.3863\n",
      "\n",
      "Epoch 16/100, Time: 14.964403629302979\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 9.0789 - G L1: 0.7693 - G logloss: 1.3863\n",
      "\n",
      "Epoch 17/100, Time: 14.951973915100098\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.6794 - G L1: 0.7293 - G logloss: 1.3863\n",
      "\n",
      "Epoch 18/100, Time: 14.929906606674194\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.6737 - G L1: 0.7287 - G logloss: 1.3863\n",
      "\n",
      "Epoch 19/100, Time: 14.937339067459106\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.4491 - G L1: 0.7063 - G logloss: 1.3863\n",
      "\n",
      "Epoch 20/100, Time: 14.96845030784607\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.3911 - G L1: 0.7005 - G logloss: 1.3863\n",
      "\n",
      "Epoch 21/100, Time: 14.959758996963501\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.0481 - G L1: 0.6662 - G logloss: 1.3863\n",
      "\n",
      "Epoch 22/100, Time: 14.945361137390137\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.0856 - G L1: 0.6699 - G logloss: 1.3863\n",
      "\n",
      "Epoch 23/100, Time: 14.92665433883667\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 8.1014 - G L1: 0.6715 - G logloss: 1.3863\n",
      "\n",
      "Epoch 24/100, Time: 14.94339895248413\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.8176 - G L1: 0.6431 - G logloss: 1.3863\n",
      "\n",
      "Epoch 25/100, Time: 14.900338172912598\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.7425 - G L1: 0.6356 - G logloss: 1.3863\n",
      "\n",
      "Epoch 26/100, Time: 14.886054039001465\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.3398 - G L1: 0.5953 - G logloss: 1.3863\n",
      "\n",
      "Epoch 27/100, Time: 14.936001539230347\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.3751 - G L1: 0.5989 - G logloss: 1.3863\n",
      "\n",
      "Epoch 28/100, Time: 14.945531129837036\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.1818 - G L1: 0.5796 - G logloss: 1.3863\n",
      "\n",
      "Epoch 29/100, Time: 14.948704719543457\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 7.1186 - G L1: 0.5732 - G logloss: 1.3863\n",
      "\n",
      "Epoch 30/100, Time: 14.892871618270874\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.9638 - G L1: 0.5578 - G logloss: 1.3863\n",
      "\n",
      "Epoch 31/100, Time: 14.926486492156982\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.9042 - G L1: 0.5518 - G logloss: 1.3863\n",
      "\n",
      "Epoch 32/100, Time: 14.937083959579468\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.8699 - G L1: 0.5484 - G logloss: 1.3863\n",
      "\n",
      "Epoch 33/100, Time: 14.924198389053345\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.9100 - G L1: 0.5524 - G logloss: 1.3863\n",
      "\n",
      "Epoch 34/100, Time: 14.940264701843262\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.9566 - G L1: 0.5570 - G logloss: 1.3863\n",
      "\n",
      "Epoch 35/100, Time: 15.022833585739136\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.6978 - G L1: 0.5312 - G logloss: 1.3863\n",
      "\n",
      "Epoch 36/100, Time: 15.023162841796875\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.6796 - G L1: 0.5293 - G logloss: 1.3863\n",
      "\n",
      "Epoch 37/100, Time: 14.964603424072266\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.4671 - G L1: 0.5081 - G logloss: 1.3863\n",
      "\n",
      "Epoch 38/100, Time: 14.934762716293335\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.3629 - G L1: 0.4977 - G logloss: 1.3863\n",
      "\n",
      "Epoch 39/100, Time: 14.947916269302368\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.4499 - G L1: 0.5064 - G logloss: 1.3863\n",
      "\n",
      "Epoch 40/100, Time: 14.94961953163147\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.2501 - G L1: 0.4864 - G logloss: 1.3863\n",
      "\n",
      "Epoch 41/100, Time: 14.944566488265991\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.4628 - G L1: 0.5077 - G logloss: 1.3863\n",
      "\n",
      "Epoch 42/100, Time: 14.953319311141968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.0935 - G L1: 0.4707 - G logloss: 1.3863\n",
      "\n",
      "Epoch 43/100, Time: 14.893331289291382\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.2382 - G L1: 0.4852 - G logloss: 1.3863\n",
      "\n",
      "Epoch 44/100, Time: 14.902809858322144\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.1116 - G L1: 0.4725 - G logloss: 1.3863\n",
      "\n",
      "Epoch 45/100, Time: 14.871660947799683\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.1044 - G L1: 0.4718 - G logloss: 1.3863\n",
      "\n",
      "Epoch 46/100, Time: 14.931531429290771\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.1415 - G L1: 0.4755 - G logloss: 1.3863\n",
      "\n",
      "Epoch 47/100, Time: 14.893040180206299\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 6.0368 - G L1: 0.4650 - G logloss: 1.3863\n",
      "\n",
      "Epoch 48/100, Time: 14.88383674621582\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.9148 - G L1: 0.4529 - G logloss: 1.3863\n",
      "\n",
      "Epoch 49/100, Time: 14.910606384277344\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.9115 - G L1: 0.4525 - G logloss: 1.3863\n",
      "\n",
      "Epoch 50/100, Time: 14.949322700500488\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.9483 - G L1: 0.4562 - G logloss: 1.3863\n",
      "\n",
      "Epoch 51/100, Time: 14.902356624603271\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.9464 - G L1: 0.4560 - G logloss: 1.3863\n",
      "\n",
      "Epoch 52/100, Time: 14.977920532226562\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.8515 - G L1: 0.4465 - G logloss: 1.3863\n",
      "\n",
      "Epoch 53/100, Time: 14.965080976486206\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.7803 - G L1: 0.4394 - G logloss: 1.3863\n",
      "\n",
      "Epoch 54/100, Time: 14.949914693832397\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.8695 - G L1: 0.4483 - G logloss: 1.3863\n",
      "\n",
      "Epoch 55/100, Time: 14.922184705734253\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.7757 - G L1: 0.4389 - G logloss: 1.3863\n",
      "\n",
      "Epoch 56/100, Time: 14.944376468658447\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.8248 - G L1: 0.4439 - G logloss: 1.3863\n",
      "\n",
      "Epoch 57/100, Time: 14.951767444610596\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.6344 - G L1: 0.4248 - G logloss: 1.3863\n",
      "\n",
      "Epoch 58/100, Time: 15.126511096954346\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.6652 - G L1: 0.4279 - G logloss: 1.3863\n",
      "\n",
      "Epoch 59/100, Time: 15.021540641784668\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.6152 - G L1: 0.4229 - G logloss: 1.3863\n",
      "\n",
      "Epoch 60/100, Time: 15.109288215637207\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.6341 - G L1: 0.4248 - G logloss: 1.3863\n",
      "\n",
      "Epoch 61/100, Time: 15.18982720375061\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4507 - G L1: 0.4064 - G logloss: 1.3863\n",
      "\n",
      "Epoch 62/100, Time: 15.044922828674316\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4800 - G L1: 0.4094 - G logloss: 1.3863\n",
      "\n",
      "Epoch 63/100, Time: 15.061362028121948\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4783 - G L1: 0.4092 - G logloss: 1.3863\n",
      "\n",
      "Epoch 64/100, Time: 15.050915479660034\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4889 - G L1: 0.4103 - G logloss: 1.3863\n",
      "\n",
      "Epoch 65/100, Time: 15.033265113830566\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.5139 - G L1: 0.4128 - G logloss: 1.3863\n",
      "\n",
      "Epoch 66/100, Time: 15.137677907943726\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4878 - G L1: 0.4101 - G logloss: 1.3863\n",
      "\n",
      "Epoch 67/100, Time: 15.043028831481934\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4387 - G L1: 0.4052 - G logloss: 1.3863\n",
      "\n",
      "Epoch 68/100, Time: 15.013300895690918\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.3562 - G L1: 0.3970 - G logloss: 1.3863\n",
      "\n",
      "Epoch 69/100, Time: 15.160648822784424\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.3404 - G L1: 0.3954 - G logloss: 1.3863\n",
      "\n",
      "Epoch 70/100, Time: 15.095310926437378\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.3667 - G L1: 0.3980 - G logloss: 1.3863\n",
      "\n",
      "Epoch 71/100, Time: 15.108312368392944\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.4034 - G L1: 0.4017 - G logloss: 1.3863\n",
      "\n",
      "Epoch 72/100, Time: 15.206947565078735\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2988 - G L1: 0.3913 - G logloss: 1.3863\n",
      "\n",
      "Epoch 73/100, Time: 15.14133358001709\n",
      "512/512 [==============================] - 15s 30ms/step - D logloss: 0.5626 - G tot: 5.3202 - G L1: 0.3934 - G logloss: 1.3863\n",
      "\n",
      "Epoch 74/100, Time: 15.232068538665771\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2543 - G L1: 0.3868 - G logloss: 1.3863\n",
      "\n",
      "Epoch 75/100, Time: 15.178027391433716\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2797 - G L1: 0.3893 - G logloss: 1.3863\n",
      "\n",
      "Epoch 76/100, Time: 15.010643720626831\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.3558 - G L1: 0.3969 - G logloss: 1.3863\n",
      "\n",
      "Epoch 77/100, Time: 15.049152374267578\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2413 - G L1: 0.3855 - G logloss: 1.3863\n",
      "\n",
      "Epoch 78/100, Time: 15.163352012634277\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2398 - G L1: 0.3854 - G logloss: 1.3863\n",
      "\n",
      "Epoch 79/100, Time: 15.037251710891724\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2237 - G L1: 0.3837 - G logloss: 1.3863\n",
      "\n",
      "Epoch 80/100, Time: 15.10335111618042\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.2068 - G L1: 0.3821 - G logloss: 1.3863\n",
      "\n",
      "Epoch 81/100, Time: 15.062972784042358\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.1800 - G L1: 0.3794 - G logloss: 1.3863\n",
      "\n",
      "Epoch 82/100, Time: 15.041741847991943\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0923 - G L1: 0.3706 - G logloss: 1.3863\n",
      "\n",
      "Epoch 83/100, Time: 15.029913187026978\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0851 - G L1: 0.3699 - G logloss: 1.3863\n",
      "\n",
      "Epoch 84/100, Time: 15.151656866073608\n",
      "512/512 [==============================] - 15s 30ms/step - D logloss: 0.5626 - G tot: 5.1288 - G L1: 0.3743 - G logloss: 1.3863\n",
      "\n",
      "Epoch 85/100, Time: 15.329052448272705\n",
      "512/512 [==============================] - 15s 30ms/step - D logloss: 0.5626 - G tot: 5.2467 - G L1: 0.3860 - G logloss: 1.3863\n",
      "\n",
      "Epoch 86/100, Time: 15.253691911697388\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.1100 - G L1: 0.3724 - G logloss: 1.3863\n",
      "\n",
      "Epoch 87/100, Time: 15.196509599685669\n",
      "512/512 [==============================] - 15s 30ms/step - D logloss: 0.5626 - G tot: 5.0442 - G L1: 0.3658 - G logloss: 1.3863\n",
      "\n",
      "Epoch 88/100, Time: 15.315067052841187\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0775 - G L1: 0.3691 - G logloss: 1.3863\n",
      "\n",
      "Epoch 89/100, Time: 15.200183391571045\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0209 - G L1: 0.3635 - G logloss: 1.3863\n",
      "\n",
      "Epoch 90/100, Time: 15.144336223602295\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0836 - G L1: 0.3697 - G logloss: 1.3863\n",
      "\n",
      "Epoch 91/100, Time: 15.182904720306396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0372 - G L1: 0.3651 - G logloss: 1.3863\n",
      "\n",
      "Epoch 92/100, Time: 15.174200296401978\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0114 - G L1: 0.3625 - G logloss: 1.3863\n",
      "\n",
      "Epoch 93/100, Time: 15.172986268997192\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0783 - G L1: 0.3692 - G logloss: 1.3863\n",
      "\n",
      "Epoch 94/100, Time: 15.039780378341675\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 4.9423 - G L1: 0.3556 - G logloss: 1.3863\n",
      "\n",
      "Epoch 95/100, Time: 15.022215127944946\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0311 - G L1: 0.3645 - G logloss: 1.3863\n",
      "\n",
      "Epoch 96/100, Time: 15.220649480819702\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 4.9121 - G L1: 0.3526 - G logloss: 1.3863\n",
      "\n",
      "Epoch 97/100, Time: 15.128490686416626\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0148 - G L1: 0.3628 - G logloss: 1.3863\n",
      "\n",
      "Epoch 98/100, Time: 15.044795513153076\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0075 - G L1: 0.3621 - G logloss: 1.3863\n",
      "\n",
      "Epoch 99/100, Time: 15.106876850128174\n",
      "512/512 [==============================] - 15s 29ms/step - D logloss: 0.5626 - G tot: 5.0082 - G L1: 0.3622 - G logloss: 1.3863\n",
      "\n",
      "Epoch 100/100, Time: 15.048355102539062\n",
      "END_TRAINING\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "END_MAIN\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
